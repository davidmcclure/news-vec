{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import gzip\n",
    "import ujson\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from glob import glob\n",
    "from itertools import islice\n",
    "from tqdm import tqdm\n",
    "from boltons.iterutils import windowed\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "from news_vec import logger\n",
    "from news_vec.title_clf import clean_headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import altair as alt\n",
    "import seaborn as sns\n",
    "\n",
    "mpl.style.use('seaborn-muted')\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Line:\n",
    "\n",
    "    def __init__(self, tokens, label, lower=True):\n",
    "        self.tokens = [t.lower() for t in tokens] if lower else tokens\n",
    "        self.label = label\n",
    "\n",
    "    def __repr__(self):\n",
    "\n",
    "        pattern = '{cls_name}<{token_count} tokens -> {label}>'\n",
    "\n",
    "        return pattern.format(\n",
    "            cls_name=self.__class__.__name__,\n",
    "            token_count=len(self.tokens),\n",
    "            label=self.label,\n",
    "        )\n",
    "    \n",
    "    def ngrams(self, n):\n",
    "        return windowed(self.tokens, n)\n",
    "                \n",
    "    def ngram_keys_iter(self, n, vocab=None):\n",
    "        for ng in self.ngrams(n):\n",
    "            if not vocab or ng in vocab:\n",
    "                yield '_' + '_'.join(ng)\n",
    "                \n",
    "    def ngram_counts_iter(self, n, vocab=None):\n",
    "        yield from Counter(self.ngram_keys_iter(n, vocab)).items()\n",
    "    \n",
    "    def features_iter(self, vocab):\n",
    "        yield from self.ngram_counts_iter(1, vocab)\n",
    "        yield from self.ngram_counts_iter(2, vocab)\n",
    "        yield from self.ngram_counts_iter(3, vocab)\n",
    "        \n",
    "    def x(self, vocab):\n",
    "        return dict(self.features_iter(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_lines(root, lower=True):\n",
    "    \"\"\"Generate links from a JSON corpus.\n",
    "\n",
    "    Yields: list<str>\n",
    "    \"\"\"\n",
    "    for path in glob('%s/*.gz' % root):\n",
    "        with gzip.open(path) as fh:\n",
    "            for line in fh:\n",
    "\n",
    "                data = ujson.loads(line)\n",
    "\n",
    "                tokens = data.get('tokens')\n",
    "                tokens = clean_headline(tokens)\n",
    "\n",
    "                if not tokens:\n",
    "                    continue\n",
    "\n",
    "                yield Line(tokens, data['label'], lower=lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus:\n",
    "\n",
    "    def __init__(self, root, skim=None, lower=True):\n",
    "        \"\"\"Read lines.\n",
    "        \"\"\"\n",
    "        logger.info('Parsing line corpus.')\n",
    "\n",
    "        lines_iter = islice(read_json_lines(root, lower), skim)\n",
    "\n",
    "        self.lines = list(tqdm(lines_iter))\n",
    "\n",
    "    def __repr__(self):\n",
    "\n",
    "        pattern = '{cls_name}<{line_count} lines>'\n",
    "\n",
    "        return pattern.format(\n",
    "            cls_name=self.__class__.__name__,\n",
    "            line_count=len(self),\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lines)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.lines)\n",
    "    \n",
    "    def ngram_counts(self, n):\n",
    "        \"\"\"Collect all ngram -> count.\n",
    "        \"\"\"\n",
    "        logger.info('Gathering %d-gram counts.' % n)\n",
    "\n",
    "        counts = Counter()\n",
    "        for line in tqdm(self):\n",
    "            counts.update(line.ngrams(n))\n",
    "\n",
    "        return counts\n",
    "\n",
    "    def topk_ngrams(self, n, k):\n",
    "        counts = self.ngram_counts(n)\n",
    "        return [ng for ng, _ in counts.most_common(k)]\n",
    "    \n",
    "    def x_iter(self, vocab):\n",
    "        for line in tqdm(self):\n",
    "            yield line.x(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-12-02 14:18:35,115 | INFO : Parsing line corpus.\n",
      "2123078it [00:52, 40332.97it/s]\n"
     ]
    }
   ],
   "source": [
    "c = Corpus('../data/titles-50k.json/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-12-02 14:22:41,082 | INFO : Gathering 1-gram counts.\n",
      "100%|██████████| 2123078/2123078 [00:20<00:00, 102873.47it/s]\n",
      "2018-12-02 14:23:01,777 | INFO : Gathering 2-gram counts.\n",
      "100%|██████████| 2123078/2123078 [00:25<00:00, 84458.04it/s]\n",
      "2018-12-02 14:23:27,719 | INFO : Gathering 3-gram counts.\n",
      "100%|██████████| 2123078/2123078 [00:23<00:00, 90478.06it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab = set(\n",
    "    c.topk_ngrams(1, 5000) +\n",
    "    c.topk_ngrams(2, 5000) +\n",
    "    c.topk_ngrams(3, 5000)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2123078/2123078 [01:06<00:00, 32075.87it/s]\n"
     ]
    }
   ],
   "source": [
    "xs = list(c.x_iter(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dv = DictVectorizer()\n",
    "X = dv.fit_transform(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2123078x15000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 23141100 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [line.label for line in c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = fit.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "            apnews.com       0.14      0.20      0.16     10032\n",
      "             bbc.co.uk       0.43      0.34      0.38     10191\n",
      "         bloomberg.com       0.32      0.20      0.24     10159\n",
      "         breitbart.com       0.22      0.16      0.18     10055\n",
      "   businessinsider.com       0.23      0.25      0.24     10207\n",
      "          buzzfeed.com       0.38      0.58      0.46      9979\n",
      "                cbc.ca       0.39      0.39      0.39     10046\n",
      "           cbsnews.com       0.15      0.08      0.10     10206\n",
      "              cnbc.com       0.21      0.25      0.23     10038\n",
      "               cnn.com       0.15      0.03      0.05     10099\n",
      "       dailycaller.com       0.25      0.17      0.20     10309\n",
      "          dailykos.com       0.26      0.33      0.29     10029\n",
      "            forbes.com       0.18      0.27      0.22     10011\n",
      "           foxnews.com       0.13      0.08      0.10     10053\n",
      "                ft.com       0.25      0.46      0.32     10298\n",
      "    huffingtonpost.com       0.13      0.09      0.11     10335\n",
      "               inc.com       0.37      0.70      0.48     10096\n",
      "     independent.co.uk       0.21      0.17      0.19     10154\n",
      "               msn.com       0.11      0.03      0.05     10093\n",
      "           nbcnews.com       0.19      0.04      0.06     10128\n",
      "          newsweek.com       0.19      0.14      0.16      9921\n",
      "               npr.org       0.19      0.09      0.13     10105\n",
      "       nydailynews.com       0.18      0.25      0.21      9865\n",
      "            nypost.com       0.20      0.24      0.22     10178\n",
      "           nytimes.com       0.23      0.08      0.12      9982\n",
      "          rawstory.com       0.22      0.33      0.26     10331\n",
      "           reuters.com       0.24      0.35      0.28     10118\n",
      "                rt.com       0.32      0.34      0.33      9884\n",
      "       sputniknews.com       0.24      0.46      0.32     10234\n",
      "       telegraph.co.uk       0.50      0.31      0.38     10051\n",
      "       theguardian.com       0.17      0.11      0.13     10187\n",
      "           thehill.com       0.18      0.29      0.22     10253\n",
      "        thetimes.co.uk       0.22      0.33      0.26      9961\n",
      "              time.com       0.25      0.09      0.14     10039\n",
      "          usatoday.com       0.27      0.34      0.30     10095\n",
      "           variety.com       0.37      0.62      0.47     10280\n",
      "              vice.com       0.19      0.36      0.25     10148\n",
      "washingtonexaminer.com       0.16      0.24      0.19     10022\n",
      "    washingtonpost.com       0.27      0.12      0.16     10315\n",
      "   washingtontimes.com       0.21      0.22      0.21     10077\n",
      "               wsj.com       0.22      0.08      0.12     10058\n",
      "             yahoo.com       0.28      0.03      0.05      9994\n",
      "\n",
      "             micro avg       0.24      0.24      0.24    424616\n",
      "             macro avg       0.24      0.24      0.22    424616\n",
      "          weighted avg       0.24      0.24      0.22    424616\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
