{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import gzip\n",
    "import ujson\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from glob import glob\n",
    "from boltons.iterutils import chunked_iter\n",
    "from itertools import islice, chain\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchtext.vocab import Vocab\n",
    "from torch.nn.utils import rnn\n",
    "from torch.nn import functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from news_vec.cuda import itype, ftype\n",
    "from news_vec import logger\n",
    "from news_vec.utils import group_by_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_lines(root, lower=True):\n",
    "    \"\"\"Read JSON corpus.\n",
    "\n",
    "    Yields: Line\n",
    "    \"\"\"\n",
    "    for path in glob('%s/*.gz' % root):\n",
    "        with gzip.open(path) as fh:\n",
    "            for line in fh:\n",
    "\n",
    "                data = ujson.loads(line)\n",
    "\n",
    "                title = data.pop('title')\n",
    "                label = data.pop('label')\n",
    "\n",
    "                yield Line(title, label, data, lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Line:\n",
    "\n",
    "    def __init__(self, text, label=None, metadata=None, lower=True):\n",
    "        self.text = text.lower() if lower else text\n",
    "        self.label = label\n",
    "        self.metadata = metadata or {}\n",
    "\n",
    "    def __repr__(self):\n",
    "\n",
    "        pattern = '{cls_name}<{char_count} chars -> {label}>'\n",
    "\n",
    "        return pattern.format(\n",
    "            cls_name=self.__class__.__name__,\n",
    "            char_count=len(self.text),\n",
    "            label=self.label,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus:\n",
    "\n",
    "    def __init__(self, root, skim=None, lower=True):\n",
    "        \"\"\"Read lines.\n",
    "        \"\"\"\n",
    "        logger.info('Parsing line corpus.')\n",
    "\n",
    "        lines_iter = islice(read_json_lines(root, lower), skim)\n",
    "\n",
    "        self.lines = list(tqdm(lines_iter))\n",
    "\n",
    "    def __repr__(self):\n",
    "\n",
    "        pattern = '{cls_name}<{line_count} lines>'\n",
    "\n",
    "        return pattern.format(\n",
    "            cls_name=self.__class__.__name__,\n",
    "            line_count=len(self),\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lines)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.lines)\n",
    "\n",
    "    def char_counts(self):\n",
    "        \"\"\"Collect all char -> count.\n",
    "        \"\"\"\n",
    "        logger.info('Gathering char counts.')\n",
    "\n",
    "        counts = Counter()\n",
    "        for line in tqdm(self):\n",
    "            counts.update(list(line.text))\n",
    "\n",
    "        return counts\n",
    "\n",
    "    def label_counts(self):\n",
    "        \"\"\"Label -> count.\n",
    "        \"\"\"\n",
    "        logger.info('Gathering label counts.')\n",
    "\n",
    "        counts = Counter()\n",
    "        for line in tqdm(self):\n",
    "            counts[line.label] += 1\n",
    "\n",
    "        return counts\n",
    "\n",
    "    def labels(self):\n",
    "        counts = self.label_counts()\n",
    "        return [label for label, _ in counts.most_common()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharEmbedding(nn.Embedding):\n",
    "\n",
    "    def __init__(self, vocab, embed_dim=15):\n",
    "        \"\"\"Set vocab, map s->i.\n",
    "        \"\"\"\n",
    "        self.vocab = vocab\n",
    "        super().__init__(len(self.vocab), embed_dim)\n",
    "\n",
    "    def chars_to_idxs(self, chars):\n",
    "        \"\"\"Map characters to embedding indexes.\n",
    "        \"\"\"\n",
    "        idxs = [self.vocab.stoi[c] for c in chars]\n",
    "\n",
    "        return torch.LongTensor(idxs).type(itype)\n",
    "\n",
    "    def forward(self, texts):\n",
    "        \"\"\"Batch-embed token chars.\n",
    "\n",
    "        Args:\n",
    "            texts (list<str>)\n",
    "        \"\"\"\n",
    "        sizes = [len(t) for t in texts]\n",
    "        chars = list(chain(*texts))\n",
    "        \n",
    "        # Map chars -> indexes.\n",
    "        x = torch.cat([self.chars_to_idxs(t) for t in chars])\n",
    "\n",
    "        # Embed.\n",
    "        x = super().forward(x)\n",
    "        \n",
    "        return group_by_sizes(x, sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LineEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size=1024, num_layers=2):\n",
    "        \"\"\"Initialize LSTM.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout()\n",
    "\n",
    "    @property\n",
    "    def out_dim(self):\n",
    "        return self.lstm.hidden_size * 2\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Sort, pack, encode, reorder.\n",
    "\n",
    "        Args:\n",
    "            x (list<Tensor>): Variable-length embedding tensors.\n",
    "        \"\"\"\n",
    "        sizes = list(map(len, x))\n",
    "\n",
    "        # Pad + LSTM.\n",
    "        x = rnn.pad_sequence(x, batch_first=True)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Unpad.\n",
    "        return [s[:size] for s, size in zip(x, sizes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharLM(nn.Module):\n",
    "\n",
    "    def __init__(self, char_counts, lstm_dim=200, embed_dim=100):\n",
    "        \"\"\"Initialize encoders + clf.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab = Vocab(char_counts)\n",
    "\n",
    "        self.embed_chars = CharEmbedding(self.vocab)\n",
    "\n",
    "        self.encode_f = LineEncoder(self.embed_chars.embedding_dim, lstm_dim)\n",
    "        self.encode_b = LineEncoder(self.embed_chars.embedding_dim, lstm_dim)\n",
    "\n",
    "        self.merge = nn.Linear(lstm_dim*2, embed_dim)\n",
    "\n",
    "        self.predict = nn.Sequential(\n",
    "            nn.Linear(embed_dim, len(self.vocab)),\n",
    "            nn.LogSoftmax(1),\n",
    "        )\n",
    "\n",
    "    def batch_iter(self, lines_iter, size=50):\n",
    "        \"\"\"Generate batches of line -> targets.\n",
    "        \"\"\"\n",
    "        for lines in chunked_iter(lines_iter, size):\n",
    "            \n",
    "            yt_idx = [self.vocab.stoi[c] for line in lines for c in line.text]\n",
    "            yt = torch.LongTensor(yt_idx).type(itype)\n",
    "\n",
    "            yield lines, yt\n",
    "\n",
    "    def encode(self, lines):\n",
    "        \"\"\"Embed lines.\n",
    "\n",
    "        Args:\n",
    "            lines (list<str>)\n",
    "        \"\"\"\n",
    "        # Add start/end spaces.\n",
    "        texts = [f' {line.text} ' for line in lines]\n",
    "        \n",
    "        x = self.embed_chars(texts)\n",
    "\n",
    "        # Forward LSTM.\n",
    "        xf = self.encode_f(x)\n",
    "\n",
    "        # Backward LSTM.\n",
    "        x_rev = [xi.flip(0) for xi in x]\n",
    "        xb = self.encode_b(x_rev)\n",
    "        xb = [xi.flip(0) for xi in xb]\n",
    "\n",
    "        # Cat [forward n-1, backward n+1] states for each token.\n",
    "        x = [\n",
    "            torch.cat([xfi[:-2], xbi[2:]], dim=1)\n",
    "            for xfi, xbi in zip(xf, xb)\n",
    "        ]\n",
    "\n",
    "        x = torch.cat(x, dim=0)\n",
    "        x = self.merge(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "\n",
    "    def __init__(self, corpus_root, lr=1e-4, batch_size=50, test_size=10000,\n",
    "        eval_every=100000, corpus_kwargs=None, model_kwargs=None):\n",
    "\n",
    "        self.corpus = Corpus(corpus_root, **(corpus_kwargs or {}))\n",
    "\n",
    "        char_counts = self.corpus.char_counts()\n",
    "\n",
    "        self.model = CharLM(char_counts, **(model_kwargs or {}))\n",
    "\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.eval_every = eval_every\n",
    "\n",
    "        self.train_lines, self.val_lines = train_test_split(\n",
    "            self.corpus.lines, test_size=test_size)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            self.model.cuda()\n",
    "\n",
    "    def train(self, epochs=10):\n",
    "        \"\"\"Train for N epochs.\n",
    "        \"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            self.train_epoch(epoch)\n",
    "\n",
    "    def train_epoch(self, epoch):\n",
    "\n",
    "        logger.info('Epoch %d' % epoch)\n",
    "\n",
    "        lines_iter = tqdm(self.train_lines)\n",
    "\n",
    "        batches = self.model.batch_iter(lines_iter, self.batch_size)\n",
    "\n",
    "        batch_losses = []\n",
    "        eval_n = 0\n",
    "        for lines, yt in batches:\n",
    "\n",
    "            self.model.train()\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            embeds = self.model.encode(lines)\n",
    "            yp = self.model.predict(embeds)\n",
    "\n",
    "            loss = F.nll_loss(yp, yt)\n",
    "            loss.backward()\n",
    "\n",
    "            self.optimizer.step()\n",
    "\n",
    "            batch_losses.append(loss.item())\n",
    "\n",
    "            n = math.floor(lines_iter.n / self.eval_every)\n",
    "\n",
    "            if n > eval_n:\n",
    "                self.log_metrics(batch_losses)\n",
    "                eval_n = n\n",
    "\n",
    "        self.log_metrics(batch_losses)\n",
    "\n",
    "    def log_metrics(self, batch_losses, n=100):\n",
    "        logger.info('Train loss: %f' % np.mean(batch_losses[-n:]))\n",
    "        self.log_val_metrics()\n",
    "\n",
    "    def log_val_metrics(self):\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        lines_iter = tqdm(self.val_lines)\n",
    "\n",
    "        batches = self.model.batch_iter(lines_iter, self.batch_size)\n",
    "\n",
    "        losses = []\n",
    "        for lines, yt in batches:\n",
    "\n",
    "            embeds = self.model.encode(lines)\n",
    "            yp = self.model.predict(embeds)\n",
    "\n",
    "            losses.append(F.nll_loss(yp, yt).item())\n",
    "\n",
    "        logger.info('Val loss: %f' % np.mean(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-12-10 01:11:48,436 | INFO : Parsing line corpus.\n",
      "10000it [00:00, 165472.75it/s]\n",
      "2018-12-10 01:11:48,499 | INFO : Gathering char counts.\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 153541.90it/s]\n"
     ]
    }
   ],
   "source": [
    "t = Trainer('../data/b13-texts.json/', eval_every=1000, test_size=100, corpus_kwargs=dict(skim=10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-12-10 01:11:52,118 | INFO : Epoch 0\n",
      " 10%|█         | 1000/9900 [00:28<03:55, 37.77it/s]2018-12-10 01:12:21,916 | INFO : Train loss: 4.418758\n",
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|█████     | 50/100 [00:00<00:00, 124.67it/s]\u001b[A\n",
      "100%|██████████| 100/100 [00:00<00:00, 115.26it/s]\u001b[A\n",
      "\u001b[A2018-12-10 01:12:22,833 | INFO : Val loss: 4.376865\n",
      " 20%|██        | 2000/9900 [00:57<03:43, 35.28it/s]2018-12-10 01:12:51,306 | INFO : Train loss: 4.349924\n",
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|█████     | 50/100 [00:00<00:00, 129.98it/s]\u001b[A\n",
      "100%|██████████| 100/100 [00:00<00:00, 122.14it/s]\u001b[A\n",
      "\u001b[A2018-12-10 01:12:52,165 | INFO : Val loss: 4.056062\n",
      " 30%|███       | 3000/9900 [01:27<03:20, 34.45it/s]2018-12-10 01:13:21,246 | INFO : Train loss: 4.085155\n",
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|█████     | 50/100 [00:00<00:00, 128.71it/s]\u001b[A\n",
      "100%|██████████| 100/100 [00:00<00:00, 121.93it/s]\u001b[A\n",
      "\u001b[A2018-12-10 01:13:22,102 | INFO : Val loss: 3.224514\n",
      " 40%|████      | 4000/9900 [01:58<02:59, 32.81it/s]2018-12-10 01:13:52,212 | INFO : Train loss: 3.856136\n",
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|█████     | 50/100 [00:00<00:00, 133.92it/s]\u001b[A\n",
      "100%|██████████| 100/100 [00:00<00:00, 124.37it/s]\u001b[A\n",
      "\u001b[A2018-12-10 01:13:53,061 | INFO : Val loss: 3.118060\n",
      " 51%|█████     | 5000/9900 [02:34<02:43, 29.99it/s]2018-12-10 01:14:28,279 | INFO : Train loss: 3.701185\n",
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|█████     | 50/100 [00:00<00:00, 133.01it/s]\u001b[A\n",
      "100%|██████████| 100/100 [00:00<00:00, 124.39it/s]\u001b[A\n",
      "\u001b[A2018-12-10 01:14:29,126 | INFO : Val loss: 3.094019\n",
      " 61%|██████    | 6000/9900 [03:05<01:57, 33.20it/s]2018-12-10 01:14:58,490 | INFO : Train loss: 3.435930\n",
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|█████     | 50/100 [00:00<00:00, 131.70it/s]\u001b[A\n",
      "100%|██████████| 100/100 [00:00<00:00, 124.22it/s]\u001b[A\n",
      "\u001b[A2018-12-10 01:14:59,331 | INFO : Val loss: 3.084599\n",
      " 71%|███████   | 7000/9900 [03:36<01:33, 31.05it/s]2018-12-10 01:15:30,136 | INFO : Train loss: 3.197932\n",
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|█████     | 50/100 [00:00<00:00, 133.58it/s]\u001b[A\n",
      "100%|██████████| 100/100 [00:00<00:00, 124.77it/s]\u001b[A\n",
      "\u001b[A2018-12-10 01:15:30,978 | INFO : Val loss: 3.079256\n",
      " 81%|████████  | 8000/9900 [04:10<00:57, 33.26it/s]2018-12-10 01:16:04,605 | INFO : Train loss: 3.105496\n",
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|█████     | 50/100 [00:00<00:00, 132.01it/s]\u001b[A\n",
      "100%|██████████| 100/100 [00:00<00:00, 124.14it/s]\u001b[A\n",
      "\u001b[A2018-12-10 01:16:05,449 | INFO : Val loss: 3.075638\n",
      " 91%|█████████ | 9000/9900 [04:40<00:26, 34.35it/s]2018-12-10 01:16:34,043 | INFO : Train loss: 3.088141\n",
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|█████     | 50/100 [00:00<00:00, 131.85it/s]\u001b[A\n",
      "100%|██████████| 100/100 [00:00<00:00, 124.16it/s]\u001b[A\n",
      "\u001b[A2018-12-10 01:16:34,886 | INFO : Val loss: 3.073099\n",
      "100%|██████████| 9900/9900 [05:07<00:00, 32.87it/s]\n",
      "2018-12-10 01:17:00,109 | INFO : Train loss: 3.080977\n",
      "100%|██████████| 100/100 [00:00<00:00, 124.11it/s]\n",
      "2018-12-10 01:17:00,949 | INFO : Val loss: 3.070087\n",
      "2018-12-10 01:17:01,003 | INFO : Epoch 1\n",
      " 10%|█         | 1000/9900 [00:29<04:14, 35.02it/s]2018-12-10 01:17:32,666 | INFO : Train loss: 3.066281\n",
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|█████     | 50/100 [00:00<00:00, 125.62it/s]\u001b[A\n",
      "100%|██████████| 100/100 [00:00<00:00, 118.05it/s]\u001b[A\n",
      "\u001b[A2018-12-10 01:17:33,552 | INFO : Val loss: 3.067443\n",
      " 20%|██        | 2000/9900 [00:59<03:43, 35.41it/s]2018-12-10 01:18:01,877 | INFO : Train loss: 3.063852\n",
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|█████     | 50/100 [00:00<00:00, 131.97it/s]\u001b[A\n",
      "100%|██████████| 100/100 [00:00<00:00, 119.96it/s]\u001b[A\n",
      "\u001b[A2018-12-10 01:18:02,767 | INFO : Val loss: 3.063506\n",
      " 30%|███       | 3000/9900 [01:29<03:20, 34.39it/s]2018-12-10 01:18:31,901 | INFO : Train loss: 3.060477\n",
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|█████     | 50/100 [00:00<00:00, 129.81it/s]\u001b[A\n",
      "100%|██████████| 100/100 [00:00<00:00, 122.60it/s]\u001b[A\n",
      "\u001b[A2018-12-10 01:18:32,753 | INFO : Val loss: 3.057849\n",
      " 40%|████      | 4000/9900 [02:00<03:03, 32.15it/s]2018-12-10 01:19:02,859 | INFO : Train loss: 3.055936\n",
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|█████     | 50/100 [00:00<00:00, 131.05it/s]\u001b[A\n",
      "100%|██████████| 100/100 [00:00<00:00, 122.17it/s]\u001b[A\n",
      "\u001b[A2018-12-10 01:19:03,720 | INFO : Val loss: 3.051000\n",
      " 51%|█████     | 5000/9900 [02:35<02:45, 29.69it/s]2018-12-10 01:19:38,646 | INFO : Train loss: 3.053447\n",
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|█████     | 50/100 [00:00<00:00, 129.88it/s]\u001b[A\n",
      "100%|██████████| 100/100 [00:00<00:00, 122.91it/s]\u001b[A\n",
      "\u001b[A2018-12-10 01:19:39,494 | INFO : Val loss: 3.040005\n",
      " 61%|██████    | 6000/9900 [03:06<01:57, 33.24it/s]2018-12-10 01:20:08,643 | INFO : Train loss: 3.046698\n",
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|█████     | 50/100 [00:00<00:00, 134.37it/s]\u001b[A\n",
      "100%|██████████| 100/100 [00:00<00:00, 124.63it/s]\u001b[A\n",
      "\u001b[A2018-12-10 01:20:09,491 | INFO : Val loss: 3.026214\n",
      " 70%|███████   | 6950/9900 [03:36<01:36, 30.43it/s]"
     ]
    }
   ],
   "source": [
    "t.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
