{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from cached_property import cached_property\n",
    "from collections import Counter, UserList, UserDict\n",
    "from tqdm import tqdm\n",
    "\n",
    "from news_vec.utils import read_json_gz_lines\n",
    "from news_vec import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Headline(UserDict):\n",
    "\n",
    "    def __repr__(self):\n",
    "\n",
    "        pattern = '{cls_name}<{token_count} tokens -> {domain}>'\n",
    "\n",
    "        return pattern.format(\n",
    "            cls_name=self.__class__.__name__,\n",
    "            token_count=len(self['clf_tokens']),\n",
    "            domain=self['domain'],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeadlineDataset(UserList):\n",
    "\n",
    "    def token_counts(self):\n",
    "        \"\"\"Collect all token -> count.\n",
    "        \"\"\"\n",
    "        logger.info('Gathering token counts.')\n",
    "\n",
    "        counts = Counter()\n",
    "        for hl, _ in tqdm(self):\n",
    "            counts.update(hl['tokens'])\n",
    "\n",
    "        return counts\n",
    "\n",
    "    def label_counts(self):\n",
    "        \"\"\"Label -> count.\n",
    "        \"\"\"\n",
    "        logger.info('Gathering label counts.')\n",
    "\n",
    "        counts = Counter()\n",
    "        for _, label in tqdm(self):\n",
    "            counts[label] += 1\n",
    "\n",
    "        return counts\n",
    "\n",
    "    def labels(self):\n",
    "        counts = self.label_counts()\n",
    "        return [label for label, _ in counts.most_common()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus:\n",
    "    \n",
    "    def __init__(self, links_root, headlines_root):\n",
    "        \"\"\"Read links df, article index.\n",
    "        \"\"\"\n",
    "        logger.info('Reading links.')\n",
    "        \n",
    "        rows = list(tqdm(read_json_gz_lines(links_root)))\n",
    "        self.links = pd.DataFrame(rows)\n",
    "        \n",
    "        logger.info('Reading headlines.')\n",
    "        \n",
    "        self.headlines = {\n",
    "            row['article_id']: Headline(row)\n",
    "            for row in tqdm(read_json_gz_lines(headlines_root))\n",
    "        }\n",
    "        \n",
    "    def make_dataset(self, df):\n",
    "        \"\"\"Index out a list of (Headline, domain) pairs.\n",
    "        \"\"\"\n",
    "        pairs = df[['article_id', 'domain']].values.tolist()\n",
    "        \n",
    "        return HeadlineDataset([\n",
    "            (self.headlines[aid], domain)\n",
    "            for aid, domain in pairs\n",
    "        ])\n",
    "        \n",
    "    @cached_property\n",
    "    def unique_articles(self):\n",
    "        return self.links[['domain', 'article_id']].drop_duplicates()\n",
    "        \n",
    "    @cached_property\n",
    "    def min_domain_count(self):\n",
    "        \"\"\"Smallest number of unique articles per domain.\n",
    "        \"\"\"\n",
    "        return self.unique_articles.groupby('domain').size().min()\n",
    "    \n",
    "    def sample_all_vs_all(self):\n",
    "        \"\"\"Sample evenly from all domains.\n",
    "        \"\"\"\n",
    "        rows = (self.unique_articles\n",
    "            .groupby('domain')\n",
    "            .apply(lambda x: x.sample(self.min_domain_count)))\n",
    "        \n",
    "        return self.make_dataset(rows)\n",
    "    \n",
    "    def sample_a_vs_b(self, a, b):\n",
    "        \"\"\"Sample evenly from two domains.\n",
    "        \"\"\"\n",
    "        rows = (self.unique_articles\n",
    "            [self.unique_articles.domain.isin([a, b])]\n",
    "            .groupby('domain')\n",
    "            .apply(lambda x: x.sample(self.min_domain_count)))\n",
    "        \n",
    "        return self.make_dataset(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-12-23 12:53:28,994 | INFO : Reading links.\n",
      "1225511it [00:03, 373703.63it/s]\n",
      "2018-12-23 12:53:34,370 | INFO : Reading headlines.\n",
      "1127502it [00:24, 45425.64it/s]\n"
     ]
    }
   ],
   "source": [
    "corpus = Corpus('../data/clf-links.json/', '../data/clf-headlines.json/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29185"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.min_domain_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ava = corpus.sample_all_vs_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "466960"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ava)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-12-23 12:54:08,716 | INFO : Gathering label counts.\n",
      "100%|██████████| 466960/466960 [00:00<00:00, 876466.11it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({'apnews.com': 29185,\n",
       "         'bloomberg.com': 29185,\n",
       "         'breitbart.com': 29185,\n",
       "         'buzzfeed.com': 29185,\n",
       "         'cnn.com': 29185,\n",
       "         'dailycaller.com': 29185,\n",
       "         'dailykos.com': 29185,\n",
       "         'foxnews.com': 29185,\n",
       "         'huffingtonpost.com': 29185,\n",
       "         'npr.org': 29185,\n",
       "         'nytimes.com': 29185,\n",
       "         'rt.com': 29185,\n",
       "         'sputniknews.com': 29185,\n",
       "         'thehill.com': 29185,\n",
       "         'washingtonpost.com': 29185,\n",
       "         'wsj.com': 29185})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ava.label_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "avb = corpus.sample_a_vs_b('nytimes.com', 'rt.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58370"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(avb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-12-23 12:54:20,121 | INFO : Gathering label counts.\n",
      "100%|██████████| 58370/58370 [00:00<00:00, 732213.75it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({'nytimes.com': 29185, 'rt.com': 29185})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avb.label_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Headline<13 tokens -> nytimes.com>, 'nytimes.com'),\n",
       " (Headline<6 tokens -> nytimes.com>, 'nytimes.com'),\n",
       " (Headline<11 tokens -> nytimes.com>, 'nytimes.com'),\n",
       " (Headline<4 tokens -> nytimes.com>, 'nytimes.com'),\n",
       " (Headline<10 tokens -> nytimes.com>, 'nytimes.com'),\n",
       " (Headline<4 tokens -> nytimes.com>, 'nytimes.com'),\n",
       " (Headline<7 tokens -> nytimes.com>, 'nytimes.com'),\n",
       " (Headline<4 tokens -> nytimes.com>, 'nytimes.com'),\n",
       " (Headline<5 tokens -> nytimes.com>, 'nytimes.com'),\n",
       " (Headline<4 tokens -> nytimes.com>, 'nytimes.com')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avb[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
